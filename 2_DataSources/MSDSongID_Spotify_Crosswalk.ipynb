{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDSongID to Sptofy Crosswalk\n",
    "\n",
    "**Purpose**: to replace song IDs from Million Song Dataset with current Spotify URI IDs  \n",
    "\n",
    "**Process**:  \n",
    "- Start with MSD  \n",
    "- Use publicly available AcousticBrainz crosswalk to find Spotify IDs  \n",
    "- For those songs that were not found in AcousticBrainz  \n",
    "  - Search Spotify API directly for Spotify URI IDs  \n",
    "  - Where multiple IDs exist, prioritize the closest matches that have song URLs  \n",
    "  \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import boto3\n",
    "import s3fs\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import time\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# SET-UP STANDARD METHOD \n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "base_dir = 'ENTER HERE'\n",
    "msd_dir = os.path.join(base_dir, 'millionsongdataset_echonest')\n",
    "spotify_dir = os.path.join(base_dir, 'spotify')\n",
    "\n",
    "# AWS S3 Boto CLient\n",
    "# AWS Credentials\n",
    "aws_id = 'X'\n",
    "aws_secret = 'Y'\n",
    "client = boto3.client('s3', \n",
    "                      aws_access_key_id = aws_id,\n",
    "                      aws_secret_access_key = aws_secret)\n",
    "\n",
    "# Spotify - Connect using lucyd Spotify app credentials  \n",
    "my_client_id = 'x'\n",
    "my_client_secret = 'y'\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id = my_client_id,\n",
    "                                                                              client_secret = my_client_secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AcousticBrainz Database of Crosswalk Files  \n",
    "`curl ftp://ftp.acousticbrainz.org/pub/acousticbrainz/acousticbrainz/labs/download/msdrosetta/millionsongdataset_echonest.tar.bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION to build FULL TRACK dictionary\n",
    "def track_dict_builder(filename):\n",
    "    with open(os.path.join(filename)) as f:\n",
    "        data_in = json.load(f)\n",
    "    \n",
    "    # Song Dict\n",
    "    data_dict_live = {}\n",
    "    if len(data_in['response']['songs']) > 0:\n",
    "        data_dict_live['id'] = data_in['response']['songs'][0]['id']\n",
    "        data_dict_live['title'] = data_in['response']['songs'][0]['title']\n",
    "        data_dict_live['artist_name'] = data_in['response']['songs'][0]['artist_name']\n",
    "\n",
    "        # Artist ID\n",
    "        for artist_foreign in sample_data['response']['songs'][0]['artist_foreign_ids']:\n",
    "            if artist_foreign['catalog'] == 'spotify':\n",
    "                data_dict_live['artist_id'] = artist_foreign['foreign_id']\n",
    "\n",
    "        # Track Crosswalk Dict\n",
    "        data_track_dict_live = {}\n",
    "        for track in data_in['response']['songs'][0]['tracks']:\n",
    "            if track['catalog'] == \"spotify\":\n",
    "                data_track_dict_live[track['id']] = track['foreign_id']\n",
    "                \n",
    "        data_dict_live['track_dict'] = data_track_dict_live\n",
    "\n",
    "    return(data_dict_live)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl to build FULL crosswalk recursively\n",
    "result_list = []\n",
    "error_list = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(msd_dir):   \n",
    "        \n",
    "    for file_loop in files:\n",
    "        try:\n",
    "            result = track_dict_builder(filename = os.path.join(subdir, file_loop))\n",
    "            if result != {}:\n",
    "                result_list.append(result)\n",
    "        except:\n",
    "            error_list.append(os.path.join(subdir, file_loop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Hits\n",
    "acousticbrainz_songid_list = []\n",
    "for obs in result_list:\n",
    "    spotify_hit_count = 0\n",
    "    for track in obs['track_dict']:\n",
    "        spotify_hit_count += 1\n",
    "    acousticbrainz_songid_list.append({'song_id':obs['id'],'ab_spotify_hit_count':spotify_hit_count})\n",
    "acousticbrainz_songid_df = pd.DataFrame(acousticbrainz_songid_list)\n",
    "ab_songid_unique_df = acousticbrainz_songid_df.groupby('song_id').agg('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in MSD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'flat_summary'\n",
    "data_location = 's3://{}/{}'.format(bucket_name, data_key)\n",
    "msd_flat = pd.read_csv(data_location)\n",
    "msd_flat_slim = msd_flat[['song_id', 'track_id', 'title', 'artist', 'spotify_uri']]\n",
    "# Drop Duplicates\n",
    "msd_prelim_clean = msd_flat_slim.drop_duplicates('song_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge AcousticBrainz on to MSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_ab_hits = msd_prelim_clean.merge(ab_songid_unique_df, \n",
    "                                     how = 'left',\n",
    "                                     on = 'song_id',\n",
    "                                     validate = 'one_to_one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify songs missing IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_missing_spotify = msd_ab_hits[pd.isnull(msd_ab_hits['spotify_uri'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to clean up search results\n",
    "def spotify_search_trim_results(full_results):\n",
    "    trim_list = []\n",
    "    for track_item in full_results['tracks']['items']:\n",
    "        trim_dict = {}\n",
    "        \n",
    "        # ALBUM\n",
    "        trim_dict['album'] = {'uri':track_item['album']['uri'],\n",
    "                              'name':track_item['album']['name']}\n",
    "\n",
    "        # ARTISTS\n",
    "        artist_list = []\n",
    "        for artist in track_item['artists']:\n",
    "            artist_list.append({'uri':artist['uri'], \n",
    "                                'name':artist['name']})\n",
    "        trim_dict['artists'] = artist_list\n",
    "\n",
    "        # TRACK\n",
    "        trim_dict['uri'] = track_item['uri']\n",
    "        trim_dict['name'] = track_item['name']\n",
    "        trim_dict['popularity'] = track_item['popularity']\n",
    "        trim_dict['preview_url'] = track_item['preview_url']\n",
    "        trim_dict['external_urls'] = {'spotify': track_item['external_urls']['spotify']}\n",
    "        \n",
    "        trim_list.append(trim_dict)\n",
    "    \n",
    "    return(trim_list)\n",
    "                          \n",
    "# Custom Function to search and save trimmed results\n",
    "def search_and_save(song_id, title_in, artist_in):\n",
    "    try:\n",
    "        # Search\n",
    "        track_search = spotify.search(\"track:\" + title_in + \" \" + \"artist:\" + artist_in)\n",
    "        # Trim\n",
    "        trim_results = spotify_search_trim_results(full_results = track_search)\n",
    "        # Save\n",
    "        with open(os.path.join(spotify_dir, song_id + '.json'), 'w') as f:\n",
    "            json.dump(trim_results, f)\n",
    "        status = \"success\" \n",
    "    except:\n",
    "        status = \"error\"\n",
    "    return(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through \n",
    "counter = 1\n",
    "for index, row in msd_missing_spotify.iterrows():\n",
    "    # Search and Save + Store Status\n",
    "    status = search_and_save(song_id = row['song_id'], \n",
    "                             title_in = row['title'], \n",
    "                             artist_in = row['artist'])\n",
    "    \n",
    "    # Save status in external JSON\n",
    "    status_dict = {row['song_id']:status}\n",
    "    with open(os.path.join(spotify_dir, 'status.json'), 'r+') as file:\n",
    "        data = json.load(file)\n",
    "        data.update(status_dict)\n",
    "        file.seek(0)\n",
    "        json.dump(data, file)\n",
    "        \n",
    "    # Pause each n loop to try to prevent Spotify from blocking me\n",
    "    if counter % 100 == 0: \n",
    "        time.sleep(1)\n",
    "        print('\\r%s' % (counter), end = \"\\r\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick ID where Multiple IDs exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the best match\n",
    "def song_id_picker(song_id_filename, song_id_lookup):\n",
    "    with open(os.path.join(spotify_dir, song_id_filename)) as f:\n",
    "        song_id_result = json.load(f)\n",
    "    song_id_in = re.sub('.json', '', song_id_filename)\n",
    "    query_artist = song_id_lookup.artist[song_id_lookup.song_id == song_id_in].tolist()[0]\n",
    "    query_title = song_id_lookup.title[song_id_lookup.song_id == song_id_in].tolist()[0]    \n",
    "    if len(song_id_result) == 0:\n",
    "        return_id = 'no_matches'\n",
    "    else:\n",
    "        spotify_results_artist = []\n",
    "        spotify_results_title = []\n",
    "        spotify_results_preview = []\n",
    "\n",
    "        for song_i in song_id_result:\n",
    "            # Grab the artist name if in the list\n",
    "            foundem = False\n",
    "            for artist_i in song_i['artists']:\n",
    "                if artist_i['name'] == query_artist:\n",
    "                    foundem = True \n",
    "                    artist_keep = artist_i['name']\n",
    "            if foundem:\n",
    "                spotify_results_artist.append(artist_keep)\n",
    "            else:\n",
    "                spotify_results_artist.append(song_i['artists'][0]['name'])\n",
    "\n",
    "            spotify_results_title.append(song_i['name'])\n",
    "            spotify_results_preview.append(song_i['preview_url'])\n",
    "\n",
    "        # Pick best\n",
    "        artist_hits = [i for i, x in enumerate([query_artist == a for a in spotify_results_artist]) if x==True]\n",
    "        title_hits = [i for i, x in enumerate([query_title == t for t in spotify_results_title]) if x==True]\n",
    "        preview_hits = [i for i, x in enumerate([p is not None for p in spotify_results_preview]) if x==True]\n",
    "    \n",
    "        try:\n",
    "            top_hit = min(set(artist_hits).intersection(set(title_hits), set(preview_hits)))\n",
    "            return_id = song_id_result[top_hit]['uri']\n",
    "        except:\n",
    "            try:\n",
    "                return_id = song_id_result[min(set(preview_hits))]['uri'] # Default to first with preview\n",
    "            except:\n",
    "                return_id = 'no_matches'\n",
    "        \n",
    "    return(return_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl to build FULL crosswalk recursively\n",
    "spotify_xwalk = []\n",
    "error_list = []\n",
    "\n",
    "counter = 1\n",
    "for subdir, dirs, files in os.walk(spotify_dir):   \n",
    "        \n",
    "    for file_loop in files:\n",
    "        try:\n",
    "            if counter > 10:\n",
    "                break\n",
    "            picked_song = song_id_picker(song_id_filename = file_loop, \n",
    "                                         song_id_lookup = msd_ab_hits)\n",
    "            spotify_xwalk.append({'song_id': re.sub('.json', '', file_loop), 'spotify_uri_search': picked_song})\n",
    "        except:\n",
    "            error_list.append(file_loop)\n",
    "            \n",
    "        if counter % 1000 == 0: \n",
    "            print('\\r%s' % (counter), end = \"\\r\")\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Final Crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_xwalk_df = pd.DataFrame(spotify_xwalk)\n",
    "msd_final_xwalk = msd_ab_hits.merge(spotify_xwalk_df, \n",
    "                                    how = 'left',\n",
    "                                    on = 'song_id',\n",
    "                                    validate = 'one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_id(row):\n",
    "    if not pd.isnull(row['spotify_uri_search']):\n",
    "        val = row['spotify_uri_search']\n",
    "        \n",
    "    elif not pd.isnull(row['spotify_uri']):\n",
    "        val = row['spotify_uri']\n",
    "    \n",
    "    else:\n",
    "        val = 'no_match'\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_final_xwalk['spotify_uri_final'] = msd_final_xwalk.apply(final_id, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to S3 for use in lucyd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Boto CLient\n",
    "# AWS Credentials\n",
    "bucket_name = 'sagemaker-msdsubset'\n",
    "csv_buffer = StringIO()\n",
    "msd_spotify_xwalk_upload.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3', aws_access_key_id=aws_id,\n",
    "                             aws_secret_access_key=aws_secret)\n",
    "s3_resource.Object(bucket_name, 'songid_spotifyuri_crosswalk.csv').put(Body=csv_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
